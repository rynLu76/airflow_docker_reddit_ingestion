[2023-02-21 21:46:25,042] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: tutorial.scraping manual__2023-02-21T21:46:20.875577+00:00 [queued]>
[2023-02-21 21:46:25,061] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: tutorial.scraping manual__2023-02-21T21:46:20.875577+00:00 [queued]>
[2023-02-21 21:46:25,063] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2023-02-21 21:46:25,065] {taskinstance.py:1239} INFO - Starting attempt 1 of 2
[2023-02-21 21:46:25,066] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2023-02-21 21:46:25,084] {taskinstance.py:1259} INFO - Executing <Task(_PythonDecoratedOperator): scraping> on 2023-02-21 21:46:20.875577+00:00
[2023-02-21 21:46:25,092] {standard_task_runner.py:52} INFO - Started process 2668 to run task
[2023-02-21 21:46:25,099] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'tutorial', 'scraping', 'manual__2023-02-21T21:46:20.875577+00:00', '--job-id', '21', '--raw', '--subdir', 'DAGS_FOLDER/reddit_ingestion_dag.py', '--cfg-path', '/tmp/tmprt5gxls9', '--error-file', '/tmp/tmplx2y0rqx']
[2023-02-21 21:46:25,101] {standard_task_runner.py:77} INFO - Job 21: Subtask scraping
[2023-02-21 21:46:25,358] {logging_mixin.py:109} INFO - Running <TaskInstance: tutorial.scraping manual__2023-02-21T21:46:20.875577+00:00 [running]> on host b46f753f04a9
[2023-02-21 21:46:25,428] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=***@example.com
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=tutorial
AIRFLOW_CTX_TASK_ID=scraping
AIRFLOW_CTX_EXECUTION_DATE=2023-02-21T21:46:20.875577+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-02-21T21:46:20.875577+00:00
[2023-02-21 21:46:25,435] {logger.py:11} INFO - ====== WebDriver manager ======
[2023-02-21 21:46:26,586] {logger.py:11} INFO - Get LATEST chromedriver version for google-chrome 110.0.5481
[2023-02-21 21:46:26,875] {logger.py:11} INFO - Driver [/home/***/.wdm/drivers/chromedriver/linux64/110.0.5481/chromedriver] found in cache
[2023-02-21 21:46:43,962] {logging_mixin.py:109} INFO - 1  --  60 random exploration encounters to spice up your session  --  UnearthedArcana
[2023-02-21 21:46:45,888] {logging_mixin.py:109} INFO - https://www.reddit.com/search/?q=starlights%20waz
[2023-02-21 21:46:48,362] {logging_mixin.py:109} INFO - 2  --  New music Friday  --  hiphopheads
[2023-02-21 21:46:49,761] {logging_mixin.py:109} INFO - https://www.reddit.com/search/?q=starlights%20waz
[2023-02-21 21:46:52,303] {logging_mixin.py:109} INFO - 3  --  Luke Skywalker  --  StarWarsEU
[2023-02-21 21:46:53,821] {logging_mixin.py:109} INFO - https://www.reddit.com/search/?q=starlights%20waz
[2023-02-21 21:46:56,046] {logging_mixin.py:109} INFO - 4  --  March 4th  --  NewMusicFridayHeads
[2023-02-21 21:46:57,155] {logging_mixin.py:109} INFO - https://www.reddit.com/search/?q=starlights%20waz
[2023-02-21 21:46:57,693] {logging_mixin.py:109} INFO - Reached to the end, no more post!
[2023-02-21 21:46:58,811] {logging_mixin.py:109} INFO -  -------------------- starlights waz --------------------
[2023-02-21 21:46:58,818] {taskinstance.py:1700} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/decorators/base.py", line 134, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/reddit_ingestion_dag.py", line 110, in scrape_test
    scrape_keyword_from_reddit(['starlights waz'])
  File "/opt/airflow/dags/scraper.py", line 155, in scrape_keyword_from_reddit
    df_post.to_csv("all_posts.csv", index=False)
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/core/generic.py", line 3482, in to_csv
    storage_options=storage_options,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/formats/format.py", line 1105, in to_csv
    csv_formatter.save()
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/formats/csvs.py", line 243, in save
    storage_options=self.storage_options,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/common.py", line 707, in get_handle
    newline="",
PermissionError: [Errno 13] Permission denied: 'all_posts.csv'
[2023-02-21 21:46:58,865] {taskinstance.py:1277} INFO - Marking task as UP_FOR_RETRY. dag_id=tutorial, task_id=scraping, execution_date=20230221T214620, start_date=20230221T214625, end_date=20230221T214658
[2023-02-21 21:46:58,889] {standard_task_runner.py:92} ERROR - Failed to execute job 21 for task scraping
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/task/task_runner/standard_task_runner.py", line 85, in _start_by_fork
    args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 298, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 107, in _run_task_by_selected_method
    _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 184, in _run_raw_task
    error_file=args.error_file,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/decorators/base.py", line 134, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/reddit_ingestion_dag.py", line 110, in scrape_test
    scrape_keyword_from_reddit(['starlights waz'])
  File "/opt/airflow/dags/scraper.py", line 155, in scrape_keyword_from_reddit
    df_post.to_csv("all_posts.csv", index=False)
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/core/generic.py", line 3482, in to_csv
    storage_options=storage_options,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/formats/format.py", line 1105, in to_csv
    csv_formatter.save()
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/formats/csvs.py", line 243, in save
    storage_options=self.storage_options,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/common.py", line 707, in get_handle
    newline="",
PermissionError: [Errno 13] Permission denied: 'all_posts.csv'
[2023-02-21 21:46:58,933] {local_task_job.py:154} INFO - Task exited with return code 1
[2023-02-21 21:46:58,976] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
