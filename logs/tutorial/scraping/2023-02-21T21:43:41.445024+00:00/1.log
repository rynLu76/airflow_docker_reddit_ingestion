[2023-02-21 21:43:45,794] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: tutorial.scraping manual__2023-02-21T21:43:41.445024+00:00 [queued]>
[2023-02-21 21:43:45,811] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: tutorial.scraping manual__2023-02-21T21:43:41.445024+00:00 [queued]>
[2023-02-21 21:43:45,813] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2023-02-21 21:43:45,814] {taskinstance.py:1239} INFO - Starting attempt 1 of 2
[2023-02-21 21:43:45,815] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2023-02-21 21:43:45,832] {taskinstance.py:1259} INFO - Executing <Task(_PythonDecoratedOperator): scraping> on 2023-02-21 21:43:41.445024+00:00
[2023-02-21 21:43:45,840] {standard_task_runner.py:52} INFO - Started process 2434 to run task
[2023-02-21 21:43:45,847] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'tutorial', 'scraping', 'manual__2023-02-21T21:43:41.445024+00:00', '--job-id', '19', '--raw', '--subdir', 'DAGS_FOLDER/reddit_ingestion_dag.py', '--cfg-path', '/tmp/tmpy4tn4fjb', '--error-file', '/tmp/tmpdr378p68']
[2023-02-21 21:43:45,849] {standard_task_runner.py:77} INFO - Job 19: Subtask scraping
[2023-02-21 21:43:46,119] {logging_mixin.py:109} INFO - Running <TaskInstance: tutorial.scraping manual__2023-02-21T21:43:41.445024+00:00 [running]> on host b46f753f04a9
[2023-02-21 21:43:46,183] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=***@example.com
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=tutorial
AIRFLOW_CTX_TASK_ID=scraping
AIRFLOW_CTX_EXECUTION_DATE=2023-02-21T21:43:41.445024+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-02-21T21:43:41.445024+00:00
[2023-02-21 21:43:46,187] {logger.py:11} INFO - ====== WebDriver manager ======
[2023-02-21 21:43:46,374] {logger.py:11} INFO - Get LATEST chromedriver version for google-chrome 110.0.5481
[2023-02-21 21:43:46,713] {logger.py:11} INFO - Driver [/home/***/.wdm/drivers/chromedriver/linux64/110.0.5481/chromedriver] found in cache
[2023-02-21 21:44:03,165] {logging_mixin.py:109} INFO - 1  --  60 random exploration encounters to spice up your session  --  UnearthedArcana
[2023-02-21 21:44:05,200] {logging_mixin.py:109} INFO - https://www.reddit.com/search/?q=starlights%20waz
[2023-02-21 21:44:13,534] {logging_mixin.py:109} INFO - 2  --  New music Friday  --  hiphopheads
[2023-02-21 21:44:14,052] {logging_mixin.py:109} INFO - https://www.reddit.com/search/?q=starlights%20waz
[2023-02-21 21:44:23,812] {logging_mixin.py:109} INFO - 3  --  Luke Skywalker  --  StarWarsEU
[2023-02-21 21:44:24,422] {logging_mixin.py:109} INFO - https://www.reddit.com/search/?q=starlights%20waz
[2023-02-21 21:44:33,958] {logging_mixin.py:109} INFO - 4  --  March 4th  --  NewMusicFridayHeads
[2023-02-21 21:44:34,212] {logging_mixin.py:109} INFO - https://www.reddit.com/search/?q=starlights%20waz
[2023-02-21 21:44:34,741] {logging_mixin.py:109} INFO - Reached to the end, no more post!
[2023-02-21 21:44:34,841] {logging_mixin.py:109} INFO -  -------------------- starlights waz --------------------
[2023-02-21 21:44:34,852] {taskinstance.py:1700} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/decorators/base.py", line 134, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/reddit_ingestion_dag.py", line 110, in scrape_test
    scrape_keyword_from_reddit(['starlights waz'])
  File "/opt/airflow/dags/scraper.py", line 155, in scrape_keyword_from_reddit
    df_post.to_csv(os.path.join(output_path, r"all_posts.csv"), index=False)
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/core/generic.py", line 3482, in to_csv
    storage_options=storage_options,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/formats/format.py", line 1105, in to_csv
    csv_formatter.save()
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/formats/csvs.py", line 243, in save
    storage_options=self.storage_options,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/common.py", line 707, in get_handle
    newline="",
FileNotFoundError: [Errno 2] No such file or directory: '/airflow_docker/output/all_posts.csv'
[2023-02-21 21:44:34,897] {taskinstance.py:1277} INFO - Marking task as UP_FOR_RETRY. dag_id=tutorial, task_id=scraping, execution_date=20230221T214341, start_date=20230221T214345, end_date=20230221T214434
[2023-02-21 21:44:34,931] {standard_task_runner.py:92} ERROR - Failed to execute job 19 for task scraping
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/task/task_runner/standard_task_runner.py", line 85, in _start_by_fork
    args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 298, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 107, in _run_task_by_selected_method
    _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 184, in _run_raw_task
    error_file=args.error_file,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/decorators/base.py", line 134, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/reddit_ingestion_dag.py", line 110, in scrape_test
    scrape_keyword_from_reddit(['starlights waz'])
  File "/opt/airflow/dags/scraper.py", line 155, in scrape_keyword_from_reddit
    df_post.to_csv(os.path.join(output_path, r"all_posts.csv"), index=False)
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/core/generic.py", line 3482, in to_csv
    storage_options=storage_options,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/formats/format.py", line 1105, in to_csv
    csv_formatter.save()
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/formats/csvs.py", line 243, in save
    storage_options=self.storage_options,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/common.py", line 707, in get_handle
    newline="",
FileNotFoundError: [Errno 2] No such file or directory: '/airflow_docker/output/all_posts.csv'
[2023-02-21 21:44:34,999] {local_task_job.py:154} INFO - Task exited with return code 1
[2023-02-21 21:44:35,047] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
