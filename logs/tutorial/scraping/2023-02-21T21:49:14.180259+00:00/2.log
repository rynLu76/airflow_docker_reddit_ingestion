[2023-02-21 21:59:26,456] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: tutorial.scraping manual__2023-02-21T21:49:14.180259+00:00 [queued]>
[2023-02-21 21:59:26,510] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: tutorial.scraping manual__2023-02-21T21:49:14.180259+00:00 [queued]>
[2023-02-21 21:59:26,515] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2023-02-21 21:59:26,518] {taskinstance.py:1239} INFO - Starting attempt 2 of 2
[2023-02-21 21:59:26,520] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2023-02-21 21:59:26,548] {taskinstance.py:1259} INFO - Executing <Task(_PythonDecoratedOperator): scraping> on 2023-02-21 21:49:14.180259+00:00
[2023-02-21 21:59:26,559] {standard_task_runner.py:52} INFO - Started process 70 to run task
[2023-02-21 21:59:26,578] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'tutorial', 'scraping', 'manual__2023-02-21T21:49:14.180259+00:00', '--job-id', '26', '--raw', '--subdir', 'DAGS_FOLDER/reddit_ingestion_dag.py', '--cfg-path', '/tmp/tmpd_3uuwlt', '--error-file', '/tmp/tmpdoys758k']
[2023-02-21 21:59:26,581] {standard_task_runner.py:77} INFO - Job 26: Subtask scraping
[2023-02-21 21:59:26,805] {logging_mixin.py:109} INFO - Running <TaskInstance: tutorial.scraping manual__2023-02-21T21:49:14.180259+00:00 [running]> on host 541881d0ad34
[2023-02-21 21:59:27,016] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=***@example.com
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=tutorial
AIRFLOW_CTX_TASK_ID=scraping
AIRFLOW_CTX_EXECUTION_DATE=2023-02-21T21:49:14.180259+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-02-21T21:49:14.180259+00:00
[2023-02-21 21:59:27,141] {logger.py:11} INFO - ====== WebDriver manager ======
[2023-02-21 21:59:27,841] {logger.py:11} INFO - Get LATEST chromedriver version for google-chrome 110.0.5481
[2023-02-21 21:59:28,572] {logger.py:11} INFO - There is no [linux64] chromedriver for browser 110.0.5481 in cache
[2023-02-21 21:59:28,579] {logger.py:11} INFO - About to download new driver from https://chromedriver.storage.googleapis.com/110.0.5481.77/chromedriver_linux64.zip
[2023-02-21 21:59:28,748] {logging_mixin.py:109} WARNING - [WDM] - Downloading:   0%|          | 0.00/7.05M [00:00<?, ?B/s]
[2023-02-21 21:59:28,856] {logging_mixin.py:109} WARNING - [WDM] - Downloading:   6%|6         | 464k/7.05M [00:00<00:01, 4.71MB/s]
[2023-02-21 21:59:28,967] {logging_mixin.py:109} WARNING - [WDM] - Downloading:  13%|#2        | 928k/7.05M [00:00<00:01, 4.42MB/s]
[2023-02-21 21:59:29,067] {logging_mixin.py:109} WARNING - [WDM] - Downloading:  48%|####7     | 3.36M/7.05M [00:00<00:00, 13.8MB/s]
[2023-02-21 21:59:29,135] {logging_mixin.py:109} WARNING - [WDM] - Downloading: 100%|##########| 7.05M/7.05M [00:00<00:00, 19.5MB/s]
[2023-02-21 21:59:29,137] {logging_mixin.py:109} WARNING - 
[2023-02-21 21:59:29,854] {logger.py:11} INFO - Driver has been saved in cache [/home/***/.wdm/drivers/chromedriver/linux64/110.0.5481]
[2023-02-21 21:59:52,280] {logging_mixin.py:109} INFO - 1  --  60 random exploration encounters to spice up your session  --  UnearthedArcana
[2023-02-21 21:59:54,535] {logging_mixin.py:109} INFO - https://www.reddit.com/search/?q=starlights%20waz
[2023-02-21 21:59:57,035] {logging_mixin.py:109} INFO - 2  --  New music Friday  --  hiphopheads
[2023-02-21 21:59:59,440] {logging_mixin.py:109} INFO - https://www.reddit.com/search/?q=starlights%20waz
[2023-02-21 22:00:02,227] {logging_mixin.py:109} INFO - 3  --  Luke Skywalker  --  StarWarsEU
[2023-02-21 22:00:03,502] {logging_mixin.py:109} INFO - https://www.reddit.com/search/?q=starlights%20waz
[2023-02-21 22:00:13,407] {logging_mixin.py:109} INFO - 4  --  March 4th  --  NewMusicFridayHeads
[2023-02-21 22:00:16,154] {logging_mixin.py:109} INFO - https://www.reddit.com/search/?q=starlights%20waz
[2023-02-21 22:00:16,925] {logging_mixin.py:109} INFO - Reached to the end, no more post!
[2023-02-21 22:00:17,407] {logging_mixin.py:109} INFO -  -------------------- starlights waz --------------------
[2023-02-21 22:00:17,520] {taskinstance.py:1700} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/decorators/base.py", line 134, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/reddit_ingestion_dag.py", line 110, in scrape_test
    scrape_keyword_from_reddit(['starlights waz'])
  File "/opt/airflow/dags/scraper.py", line 155, in scrape_keyword_from_reddit
    df_post.to_csv(os.path.join(output_path, r"all_posts.csv"), index=False)
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/core/generic.py", line 3482, in to_csv
    storage_options=storage_options,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/formats/format.py", line 1105, in to_csv
    csv_formatter.save()
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/formats/csvs.py", line 243, in save
    storage_options=self.storage_options,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/common.py", line 707, in get_handle
    newline="",
FileNotFoundError: [Errno 2] No such file or directory: '/airflow_docker/output/all_posts.csv'
[2023-02-21 22:00:17,801] {taskinstance.py:1277} INFO - Marking task as FAILED. dag_id=tutorial, task_id=scraping, execution_date=20230221T214914, start_date=20230221T215926, end_date=20230221T220017
[2023-02-21 22:00:17,941] {standard_task_runner.py:92} ERROR - Failed to execute job 26 for task scraping
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/task/task_runner/standard_task_runner.py", line 85, in _start_by_fork
    args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 298, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 107, in _run_task_by_selected_method
    _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 184, in _run_raw_task
    error_file=args.error_file,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/decorators/base.py", line 134, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/reddit_ingestion_dag.py", line 110, in scrape_test
    scrape_keyword_from_reddit(['starlights waz'])
  File "/opt/airflow/dags/scraper.py", line 155, in scrape_keyword_from_reddit
    df_post.to_csv(os.path.join(output_path, r"all_posts.csv"), index=False)
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/core/generic.py", line 3482, in to_csv
    storage_options=storage_options,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/formats/format.py", line 1105, in to_csv
    csv_formatter.save()
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/formats/csvs.py", line 243, in save
    storage_options=self.storage_options,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/common.py", line 707, in get_handle
    newline="",
FileNotFoundError: [Errno 2] No such file or directory: '/airflow_docker/output/all_posts.csv'
[2023-02-21 22:00:18,268] {local_task_job.py:154} INFO - Task exited with return code 1
[2023-02-21 22:00:18,365] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
