[2023-02-21 22:00:39,907] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: tutorial.scraping manual__2023-02-21T22:00:33.857277+00:00 [queued]>
[2023-02-21 22:00:39,933] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: tutorial.scraping manual__2023-02-21T22:00:33.857277+00:00 [queued]>
[2023-02-21 22:00:39,934] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2023-02-21 22:00:39,936] {taskinstance.py:1239} INFO - Starting attempt 1 of 2
[2023-02-21 22:00:39,938] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2023-02-21 22:00:39,960] {taskinstance.py:1259} INFO - Executing <Task(_PythonDecoratedOperator): scraping> on 2023-02-21 22:00:33.857277+00:00
[2023-02-21 22:00:39,971] {standard_task_runner.py:52} INFO - Started process 255 to run task
[2023-02-21 22:00:39,982] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'tutorial', 'scraping', 'manual__2023-02-21T22:00:33.857277+00:00', '--job-id', '28', '--raw', '--subdir', 'DAGS_FOLDER/reddit_ingestion_dag.py', '--cfg-path', '/tmp/tmpxoybd7cd', '--error-file', '/tmp/tmpxp4elvkm']
[2023-02-21 22:00:39,985] {standard_task_runner.py:77} INFO - Job 28: Subtask scraping
[2023-02-21 22:00:40,125] {logging_mixin.py:109} INFO - Running <TaskInstance: tutorial.scraping manual__2023-02-21T22:00:33.857277+00:00 [running]> on host 541881d0ad34
[2023-02-21 22:00:40,419] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=***@example.com
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=tutorial
AIRFLOW_CTX_TASK_ID=scraping
AIRFLOW_CTX_EXECUTION_DATE=2023-02-21T22:00:33.857277+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-02-21T22:00:33.857277+00:00
[2023-02-21 22:00:40,425] {logger.py:11} INFO - ====== WebDriver manager ======
[2023-02-21 22:00:40,651] {logger.py:11} INFO - Get LATEST chromedriver version for google-chrome 110.0.5481
[2023-02-21 22:00:40,997] {logger.py:11} INFO - Driver [/home/***/.wdm/drivers/chromedriver/linux64/110.0.5481/chromedriver] found in cache
[2023-02-21 22:00:59,449] {logging_mixin.py:109} INFO - 1  --  60 random exploration encounters to spice up your session  --  UnearthedArcana
[2023-02-21 22:01:01,511] {logging_mixin.py:109} INFO - https://www.reddit.com/search/?q=starlights%20waz
[2023-02-21 22:01:09,917] {logging_mixin.py:109} INFO - 2  --  New music Friday  --  hiphopheads
[2023-02-21 22:01:11,368] {logging_mixin.py:109} INFO - https://www.reddit.com/search/?q=starlights%20waz
[2023-02-21 22:01:20,284] {logging_mixin.py:109} INFO - 3  --  Luke Skywalker  --  StarWarsEU
[2023-02-21 22:01:20,628] {logging_mixin.py:109} INFO - https://www.reddit.com/search/?q=starlights%20waz
[2023-02-21 22:01:30,499] {logging_mixin.py:109} INFO - 4  --  March 4th  --  NewMusicFridayHeads
[2023-02-21 22:01:31,141] {logging_mixin.py:109} INFO - https://www.reddit.com/search/?q=starlights%20waz
[2023-02-21 22:01:31,689] {logging_mixin.py:109} INFO - Reached to the end, no more post!
[2023-02-21 22:01:31,835] {logging_mixin.py:109} INFO -  -------------------- starlights waz --------------------
[2023-02-21 22:01:31,855] {taskinstance.py:1700} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/decorators/base.py", line 134, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/reddit_ingestion_dag.py", line 110, in scrape_test
    scrape_keyword_from_reddit(['starlights waz'])
  File "/opt/airflow/dags/scraper.py", line 155, in scrape_keyword_from_reddit
    df_post.to_csv(os.path.join(output_path, r"all_posts.csv"), index=False)
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/core/generic.py", line 3482, in to_csv
    storage_options=storage_options,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/formats/format.py", line 1105, in to_csv
    csv_formatter.save()
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/formats/csvs.py", line 243, in save
    storage_options=self.storage_options,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/common.py", line 707, in get_handle
    newline="",
FileNotFoundError: [Errno 2] No such file or directory: '/airflow_docker/output/all_posts.csv'
[2023-02-21 22:01:31,922] {taskinstance.py:1277} INFO - Marking task as UP_FOR_RETRY. dag_id=tutorial, task_id=scraping, execution_date=20230221T220033, start_date=20230221T220039, end_date=20230221T220131
[2023-02-21 22:01:31,953] {standard_task_runner.py:92} ERROR - Failed to execute job 28 for task scraping
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/task/task_runner/standard_task_runner.py", line 85, in _start_by_fork
    args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 298, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 107, in _run_task_by_selected_method
    _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 184, in _run_raw_task
    error_file=args.error_file,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/decorators/base.py", line 134, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/reddit_ingestion_dag.py", line 110, in scrape_test
    scrape_keyword_from_reddit(['starlights waz'])
  File "/opt/airflow/dags/scraper.py", line 155, in scrape_keyword_from_reddit
    df_post.to_csv(os.path.join(output_path, r"all_posts.csv"), index=False)
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/core/generic.py", line 3482, in to_csv
    storage_options=storage_options,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/formats/format.py", line 1105, in to_csv
    csv_formatter.save()
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/formats/csvs.py", line 243, in save
    storage_options=self.storage_options,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/common.py", line 707, in get_handle
    newline="",
FileNotFoundError: [Errno 2] No such file or directory: '/airflow_docker/output/all_posts.csv'
[2023-02-21 22:01:32,000] {local_task_job.py:154} INFO - Task exited with return code 1
[2023-02-21 22:01:32,198] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
